[32m[I 2025-07-03 15:10:36,423][0m A new study created in RDB with name: maddpg_balance_tuning_wandb102[0m
--------------------------------------------------
Episode: 500, average_score: -26.57, delta: -26.57
--------------------------------------------------
/Users/brian/Reinforcement_learning/Luciano/RL_Multi_Agent/Distill.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.states = T.stack([T.tensor(s[agent_idx], dtype=T.float32) for s, _ in data])
/Users/brian/Reinforcement_learning/Luciano/RL_Multi_Agent/Distill.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.actions = T.stack([T.tensor(a[agent_idx], dtype=T.float32) for _, a in data])
[32m[I 2025-07-03 15:18:14,332][0m Trial 0 finished with value: 0.0 and parameters: {'H': 0.12072347451780086, 'decay_rate': 0.9905692580889445, 'alpha': 4.011717542104359e-05, 'beta': 0.0007209867035891705, 'batch_size': 1024, 'learn_every': 200, 'gamma': 0.865741841997969, 'tau': 0.049299284386038256}. Best is trial 0 with value: 0.0.[0m
--------------------------------------------------
Episode: 500, average_score: -29.11, delta: -29.11
--------------------------------------------------
[32m[I 2025-07-03 15:31:15,206][0m Trial 1 finished with value: 0.0 and parameters: {'H': 0.10218270005088449, 'decay_rate': 0.9918458157275097, 'alpha': 2.8952328321529533e-05, 'beta': 0.001916552813775962, 'batch_size': 1024, 'learn_every': 50, 'gamma': 0.8154118749114613, 'tau': 0.014328038052809747}. Best is trial 0 with value: 0.0.[0m
--------------------------------------------------
Episode: 500, average_score: -18.42, delta: -18.42
--------------------------------------------------
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
... Saving checkpoint ...
[32m[I 2025-07-03 15:37:17,681][0m Trial 2 finished with value: 8.253133773803711 and parameters: {'H': 0.1162005188087121, 'decay_rate': 0.9976518726902592, 'alpha': 3.9108417749715766e-05, 'beta': 0.00028515739949334587, 'batch_size': 512, 'learn_every': 400, 'gamma': 0.9869694102001108, 'tau': 0.05289878649443732}. Best is trial 2 with value: 8.253133773803711.[0m
--------------------------------------------------
Episode: 500, average_score: -37.93, delta: -37.93
--------------------------------------------------
/Users/brian/Reinforcement_learning/Luciano/RL_Multi_Agent/Distill.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.states = T.stack([T.tensor(s[agent_idx], dtype=T.float32) for s, _ in data])
/Users/brian/Reinforcement_learning/Luciano/RL_Multi_Agent/Distill.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.actions = T.stack([T.tensor(a[agent_idx], dtype=T.float32) for _, a in data])
[32m[I 2025-07-03 15:45:23,547][0m Trial 3 finished with value: 0.0 and parameters: {'H': 0.20210600601204107, 'decay_rate': 0.9957494353335169, 'alpha': 9.709048256457457e-06, 'beta': 0.00012897392435579324, 'batch_size': 64, 'learn_every': 400, 'gamma': 0.9271007513798798, 'tau': 0.056710555861730384}. Best is trial 2 with value: 8.253133773803711.[0m
--------------------------------------------------
Episode: 500, average_score: -38.01, delta: -38.01
--------------------------------------------------
[32m[I 2025-07-03 15:55:06,710][0m Trial 4 finished with value: 0.0 and parameters: {'H': 0.03949952761280354, 'decay_rate': 0.9903050317905189, 'alpha': 1.6759211539476123e-06, 'beta': 0.004829762747764912, 'batch_size': 64, 'learn_every': 200, 'gamma': 0.9157269413095249, 'tau': 0.0990492454814105}. Best is trial 2 with value: 8.253133773803711.[0m
--------------------------------------------------
Episode: 500, average_score: -33.91, delta: -33.91
--------------------------------------------------
[32m[I 2025-07-03 16:00:00,346][0m Trial 5 finished with value: 0.0 and parameters: {'H': 0.014617110339049244, 'decay_rate': 0.999847807899966, 'alpha': 0.0007964118662121728, 'beta': 1.1426149117884472e-05, 'batch_size': 512, 'learn_every': 100, 'gamma': 0.9944941006347141, 'tau': 0.05615875629502275}. Best is trial 2 with value: 8.253133773803711.[0m
--------------------------------------------------
Episode: 500, average_score: -26.79, delta: -26.79
--------------------------------------------------
[32m[I 2025-07-03 16:04:36,308][0m Trial 6 finished with value: 0.0 and parameters: {'H': 0.2805026855951151, 'decay_rate': 0.9983273971092108, 'alpha': 0.00018525329308024117, 'beta': 0.00011970207072524946, 'batch_size': 256, 'learn_every': 400, 'gamma': 0.9985311095202969, 'tau': 0.019444762423369087}. Best is trial 2 with value: 8.253133773803711.[0m
--------------------------------------------------
Episode: 500, average_score: -36.67, delta: -36.67
--------------------------------------------------
[32m[I 2025-07-03 16:12:12,648][0m Trial 7 finished with value: 0.0 and parameters: {'H': 0.473751646034538, 'decay_rate': 0.9959131598874607, 'alpha': 1.998966455194991e-06, 'beta': 2.5418570734670746e-05, 'batch_size': 128, 'learn_every': 400, 'gamma': 0.9539881217219152, 'tau': 0.08729574710701671}. Best is trial 2 with value: 8.253133773803711.[0m

Number of finished trials: 8

Best trial:
  Value: 8.253133773803711
  Params:
    H: 0.1162005188087121
    decay_rate: 0.9976518726902592
    alpha: 3.9108417749715766e-05
    beta: 0.00028515739949334587
    batch_size: 512
    learn_every: 400
    gamma: 0.9869694102001108
    tau: 0.05289878649443732
  User attrs:
    H: 0.1162005188087121
    alpha: 3.9108417749715766e-05
    batch_size: 512
    beta: 0.00028515739949334587
    decay_rate: 0.9976518726902592
    gamma: 0.9869694102001108
    learn_every: 400
    tau: 0.05289878649443732
Traceback (most recent call last):
  File "/Users/brian/miniconda3/envs/RL/lib/python3.11/site-packages/optuna/visualization/_plotly_imports.py", line 7, in <module>
    import plotly
ModuleNotFoundError: No module named 'plotly'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/brian/Reinforcement_learning/Luciano/RL_Multi_Agent/tuning.py", line 143, in <module>
    optuna.visualization.plot_optimization_history(study).show()
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/brian/miniconda3/envs/RL/lib/python3.11/site-packages/optuna/visualization/_optimization_history.py", line 200, in plot_optimization_history
    _imports.check()
  File "/Users/brian/miniconda3/envs/RL/lib/python3.11/site-packages/optuna/_imports.py", line 94, in check
    raise ImportError(message) from exc_value
ImportError: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.
